import apiClient from './api.js'

/**
 * Smart Data Import Service - Handles intelligent file upload with automatic categorization
 * S·ª≠ d·ª•ng API SmartDataImport ƒë·ªÉ t·ª± ƒë·ªông ph√¢n lo·∫°i v√† import d·ªØ li·ªáu
 * H·ªó tr·ª£ chunked upload cho file l·ªõn v√† progress tracking
 */
class SmartImportService {

  constructor() {
    // C·∫•u h√¨nh chunked upload
    this.CHUNK_SIZE = 1024 * 1024 * 2 // 2MB m·ªói chunk
    this.MAX_FILE_SIZE = 1024 * 1024 * 100 // 100MB max file size
    this.LARGE_FILE_THRESHOLD = 1024 * 1024 * 10 // 10MB ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ d√πng chunked upload
  }

  /**
   * Upload file v·ªõi Smart Import - t·ª± ƒë·ªông ph√¢n lo·∫°i d·ª±a v√†o filename
   * @param {File} file - File c·∫ßn upload
   * @param {Date} statementDate - Ng√†y sao k√™ (t√πy ch·ªçn)
   * @param {Function} progressCallback - Callback ƒë·ªÉ track progress
   * @returns {Promise} K·∫øt qu·∫£ upload v√† x·ª≠ l√Ω
   */
  async uploadSmartFile(file, statementDate = null, progressCallback = null) {
    try {
      // Validation file size
      if (file.size > this.MAX_FILE_SIZE) {
        throw new Error(`File ${file.name} qu√° l·ªõn (${this.formatFileSize(file.size)}). Gi·ªõi h·∫°n t·ªëi ƒëa: ${this.formatFileSize(this.MAX_FILE_SIZE)}`)
      }

      // Quy·∫øt ƒë·ªãnh d√πng chunked upload hay normal upload
      if (file.size > this.LARGE_FILE_THRESHOLD) {
        console.log(`üì¶ Using chunked upload for large file: ${file.name} (${this.formatFileSize(file.size)})`)
        return await this.uploadLargeFile(file, statementDate, progressCallback)
      } else {
        console.log(`üì§ Using normal upload for file: ${file.name} (${this.formatFileSize(file.size)})`)
        return await this.uploadNormalFile(file, statementDate, progressCallback)
      }
    } catch (error) {
      console.error('üî• Smart Import upload error:', error)
      throw new Error(`Smart Import failed: ${error.response?.data?.message || error.message}`)
    }
  }

  /**
   * Upload file th√¥ng th∆∞·ªùng (nh·ªè h∆°n 10MB) - OPTIMIZED
   */
  async uploadNormalFile(file, statementDate = null, progressCallback = null) {
    const formData = new FormData()
    formData.append('file', file)

    // Th√™m statement date n·∫øu c√≥
    if (statementDate) {
      formData.append('statementDate', statementDate.toISOString())
    }

    const response = await apiClient.post('/SmartDataImport/upload', formData, {
      headers: {
        'Content-Type': 'multipart/form-data',
        // üöÄ OPTIMIZATION: Enable compression
        'Accept-Encoding': 'gzip, deflate, br'
      },
      timeout: 120000, // üöÄ Gi·∫£m timeout xu·ªëng 2 ph√∫t cho file nh·ªè
      onUploadProgress: (progressEvent) => {
        if (progressCallback && progressEvent.total) {
          const percentCompleted = Math.round((progressEvent.loaded * 100) / progressEvent.total)
          progressCallback({
            loaded: progressEvent.loaded,
            total: progressEvent.total,
            percentage: percentCompleted,
            stage: 'uploading'
          })
        }
      }
    })

    return response.data
  }

  /**
   * Upload file l·ªõn v·ªõi chunked upload - OPTIMIZED
   */
  async uploadLargeFile(file, statementDate = null, progressCallback = null) {
    // TODO: Implement true chunked upload when backend supports it
    // For now, use optimized upload with extended timeout
    const formData = new FormData()
    formData.append('file', file)

    if (statementDate) {
      formData.append('statementDate', statementDate.toISOString())
    }

    const response = await apiClient.post('/SmartDataImport/upload', formData, {
      headers: {
        'Content-Type': 'multipart/form-data',
        // üöÄ OPTIMIZATION: Enable compression
        'Accept-Encoding': 'gzip, deflate, br'
      },
      timeout: 300000, // üöÄ 5 ph√∫t cho file l·ªõn
      onUploadProgress: (progressEvent) => {
        if (progressCallback && progressEvent.total) {
          const percentCompleted = Math.round((progressEvent.loaded * 100) / progressEvent.total)
          progressCallback({
            loaded: progressEvent.loaded,
            total: progressEvent.total,
            percentage: percentCompleted,
            stage: 'uploading'
          })
        }
      }
    })

    return response.data
  }

  /**
   * Format file size th√†nh human readable
   */
  formatFileSize(bytes) {
    if (bytes === 0) return '0 Bytes'
    const k = 1024
    const sizes = ['Bytes', 'KB', 'MB', 'GB']
    const i = Math.floor(Math.log(bytes) / Math.log(k))
    return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i]
  }

  /**
   * Upload nhi·ªÅu file v·ªõi Smart Import - OPTIMIZED PARALLEL VERSION
   * @param {FileList|Array} files - Danh s√°ch file c·∫ßn upload
   * @param {Date} statementDate - Ng√†y sao k√™ (t√πy ch·ªçn)
   * @param {Function} progressCallback - Callback ƒë·ªÉ track progress t·ªïng th·ªÉ
   * @returns {Promise} K·∫øt qu·∫£ upload batch
   */
  async uploadSmartFiles(files, statementDate = null, progressCallback = null) {
    try {
      const totalFiles = files.length
      const MAX_CONCURRENT_UPLOADS = 3 // üöÄ Upload t·ªëi ƒëa 3 file c√πng l√∫c

      console.log(`üöÄ Starting PARALLEL Smart Import with ${totalFiles} files (max ${MAX_CONCURRENT_UPLOADS} concurrent)`)

      // üìä Tracking variables
      const results = []
      const progressTracking = new Map() // Track progress c·ªßa t·ª´ng file
      let completedCount = 0

      // Update overall progress function
      const updateOverallProgress = () => {
        const totalProgress = Array.from(progressTracking.values()).reduce((sum, progress) => sum + progress, 0)
        const overallPercentage = Math.round(totalProgress / totalFiles)

        if (progressCallback) {
          const currentFiles = Array.from(progressTracking.keys()).filter(fileName =>
            progressTracking.get(fileName) < 100
          )

          progressCallback({
            current: completedCount,
            total: totalFiles,
            percentage: overallPercentage,
            currentFile: currentFiles.length > 0 ? `${currentFiles.length} file(s) ƒëang upload...` : 'Ho√†n th√†nh',
            stage: completedCount === totalFiles ? 'completed' : 'uploading',
            activeFiles: currentFiles.length
          })
        }
      }

      // üöÄ Create upload promises v·ªõi concurrency control
      const uploadFile = async (file, index) => {
        try {
          // Initialize progress tracking
          progressTracking.set(file.name, 0)

          // Progress callback cho t·ª´ng file
          const fileProgressCallback = (fileProgress) => {
            progressTracking.set(file.name, fileProgress.percentage)
            updateOverallProgress()
          }

          const result = await this.uploadSmartFile(file, statementDate, fileProgressCallback)

          // Mark as completed
          progressTracking.set(file.name, 100)
          completedCount++

          console.log(`‚úÖ Successfully uploaded: ${file.name} (${completedCount}/${totalFiles})`)

          return {
            fileName: file.name,
            success: true,
            result: result,
            index: index + 1,
            fileSize: file.size
          }
        } catch (error) {
          completedCount++
          console.error(`‚ùå Failed to upload: ${file.name}`, error)

          return {
            fileName: file.name,
            success: false,
            error: error.message,
            index: index + 1,
            fileSize: file.size
          }
        }
      }

      // üöÄ PARALLEL PROCESSING v·ªõi concurrency limit
      const promises = []
      for (let i = 0; i < totalFiles; i += MAX_CONCURRENT_UPLOADS) {
        const batch = []

        // T·∫°o batch c·ªßa MAX_CONCURRENT_UPLOADS files
        for (let j = 0; j < MAX_CONCURRENT_UPLOADS && (i + j) < totalFiles; j++) {
          const fileIndex = i + j
          const file = files[fileIndex]
          batch.push(uploadFile(file, fileIndex))
        }

        // Ch·ªù batch ho√†n th√†nh tr∆∞·ªõc khi ti·∫øp t·ª•c batch ti·∫øp theo
        const batchResults = await Promise.all(batch)
        results.push(...batchResults)

        console.log(`üì¶ Completed batch ${Math.floor(i / MAX_CONCURRENT_UPLOADS) + 1} - ${batchResults.length} files`)
      }

      // Final progress update
      if (progressCallback) {
        progressCallback({
          current: totalFiles,
          total: totalFiles,
          percentage: 100,
          currentFile: 'Ho√†n th√†nh t·∫•t c·∫£',
          stage: 'completed',
          activeFiles: 0
        })
      }

      const successCount = results.filter(r => r.success).length
      const failureCount = results.filter(r => !r.success).length

      console.log(`üèÅ PARALLEL Smart Import completed: ${successCount}/${totalFiles} successful`)

      return {
        totalFiles: totalFiles,
        successCount: successCount,
        failureCount: failureCount,
        results: results,
        totalSize: Array.from(files).reduce((sum, file) => sum + file.size, 0),
        uploadMethod: 'parallel',
        maxConcurrency: MAX_CONCURRENT_UPLOADS
      }
    } catch (error) {
      console.error('üî• Smart Import batch upload error:', error)
      throw new Error(`Smart Import batch failed: ${error.message}`)
    }
  }

  /**
   * L·∫•y danh s√°ch file ƒë√£ upload v√† k·∫øt qu·∫£ x·ª≠ l√Ω
   * @returns {Promise} Danh s√°ch import records
   */
  async getImportedRecords() {
    try {
      const response = await apiClient.get('/SmartDataImport/imported-records')
      return response.data
    } catch (error) {
      console.error('üî• Error fetching imported records:', error)
      throw new Error(`Fetch imported records failed: ${error.response?.data?.message || error.message}`)
    }
  }

  /**
   * L·∫•y chi ti·∫øt m·ªôt import record
   * @param {number} recordId - ID c·ªßa import record
   * @returns {Promise} Chi ti·∫øt import record
   */
  async getImportedRecordDetail(recordId) {
    try {
      const response = await apiClient.get(`/SmartDataImport/imported-records/${recordId}`)
      return response.data
    } catch (error) {
      console.error('üî• Error fetching import record detail:', error)
      throw new Error(`Fetch import record detail failed: ${error.response?.data?.message || error.message}`)
    }
  }

  /**
   * Process imported data to history tables (move to raw data tables)
   * @param {number} recordId - ID c·ªßa import record
   * @param {string} category - Category c·ªßa data
   * @param {Date} statementDate - Ng√†y sao k√™
   * @returns {Promise} K·∫øt qu·∫£ processing
   */
  async processToHistoryTables(recordId, category, statementDate = null) {
    try {
      const payload = {
        importedDataRecordId: recordId,
        category: category
      }

      if (statementDate) {
        payload.statementDate = statementDate.toISOString()
      }

      const response = await apiClient.post('/SmartDataImport/process-to-history', payload)
      return response.data
    } catch (error) {
      console.error('üî• Error processing to history tables:', error)
      throw new Error(`Process to history failed: ${error.response?.data?.message || error.message}`)
    }
  }

  /**
   * Validate category support
   * @returns {Promise} Danh s√°ch category ƒë∆∞·ª£c h·ªó tr·ª£
   */
  async getSupportedCategories() {
    try {
      const response = await apiClient.get('/SmartDataImport/supported-categories')
      return response.data
    } catch (error) {
      console.error('üî• Error fetching supported categories:', error)
      return ['DP01', 'LN01', 'LN02', 'LN03', 'GL01', 'GL41', 'DB01', 'DPDA', 'EI01', 'KH03', 'RR01', 'DT_KHKD1'] // fallback
    }
  }

  /**
   * T·ª± ƒë·ªông detect category t·ª´ filename
   * @param {string} fileName - T√™n file
   * @returns {string} Category ƒë∆∞·ª£c detect
   */
  detectCategoryFromFileName(fileName) {
    const upperFileName = fileName.toUpperCase()

    // Pattern matching for different data types
    const patterns = {
      'DP01': /DP01|DEPOSIT|TIENGUI/,
      'LN01': /LN01|LOAN|CHOAVAY/,
      'LN02': /LN02|LOAN.*SCHEDULE|LICHTRANGOAN/,
      'LN03': /LN03|LOAN.*CLASSIFY|PHANLOAINO/,
      'GL01': /GL01|GENERAL.*LEDGER|SOTONGOHAP/,
      'GL41': /GL41|BALANCE|BANGSODUE/,
      'DB01': /DB01|COLLATERAL|TAISANDAMBAO/,
      'DPDA': /DPDA|DEPOSIT.*DETAIL|CHITIETTIENGUI/,
      'EI01': /EI01|INCOME|THUNHAP/,
      'KH03': /KH03|CUSTOMER|KHACHHANG/,
      'RR01': /RR01|RATIO|TYLE/,
      'DT_KHKD1': /DT_KHKD1|7800|KHKD|KINHKINHDOANH/
    }

    for (const [category, pattern] of Object.entries(patterns)) {
      if (pattern.test(upperFileName)) {
        return category
      }
    }

    return 'UNKNOWN'
  }

  /**
   * Extract NgayDL from filename
   * @param {string} fileName - T√™n file
   * @returns {Date|null} Ng√†y DL ƒë∆∞·ª£c extract
   */
  extractDateFromFileName(fileName) {
    const patterns = [
      /(\d{4})(\d{2})(\d{2})/,  // YYYYMMDD
      /(\d{2})(\d{2})(\d{4})/,  // DDMMYYYY
      /(\d{4})-(\d{2})-(\d{2})/, // YYYY-MM-DD
      /(\d{2})-(\d{2})-(\d{4})/  // DD-MM-YYYY
    ]

    for (const pattern of patterns) {
      const match = fileName.match(pattern)
      if (match) {
        let year, month, day
        if (pattern.source.includes('(\\d{4})') && match[1].length === 4) {
          // YYYY first
          year = parseInt(match[1])
          month = parseInt(match[2])
          day = parseInt(match[3])
        } else {
          // DD first
          day = parseInt(match[1])
          month = parseInt(match[2])
          year = parseInt(match[3])
        }

        if (year > 2000 && year < 2100 && month >= 1 && month <= 12 && day >= 1 && day <= 31) {
          return new Date(year, month - 1, day)
        }
      }
    }

    return null
  }
}

// Export singleton instance
export default new SmartImportService()
