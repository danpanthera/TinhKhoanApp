import apiClient from './api.js'
import ChunkedUploadService from './chunkedUploadService.js'

/**
 * üöÄ Enhanced Data Import Service - Multiple upload strategies for optimal performance
 */
class DataImportService {
  /**
   * Upload a file to the server using Direct Import (tƒÉng t·ªëc 2-5x)
   * @param {File} file - The file to upload
   * @param {string} category - The category of data (kh√¥ng d√πng n·ªØa)
   * @param {Function} onProgress - Progress callback function
   * @returns {Promise} Upload result
   */
  async uploadFile(file, category = 'General', onProgress = null) {
    try {
      const formData = new FormData()
      formData.append('file', file)

      // üöÄ Enhanced config for large files
      const config = {
        headers: {
          'Content-Type': 'multipart/form-data',
        },
        timeout: 900000, // 15 ph√∫t cho file r·∫•t l·ªõn
        maxContentLength: Infinity,
        maxBodyLength: Infinity,
        // üìä Progress tracking
        onUploadProgress: progressEvent => {
          if (onProgress && progressEvent.total) {
            const percentCompleted = Math.round((progressEvent.loaded * 100) / progressEvent.total)
            onProgress(percentCompleted, progressEvent.loaded, progressEvent.total)
          }
        },
      }

      console.log(`üöÄ Starting upload for ${file.name} (${(file.size / 1024 / 1024).toFixed(2)} MB)`)

      // üöÄ S·ª≠ d·ª•ng Direct Import API - tƒÉng t·ªëc 2-5x
      const response = await apiClient.post('/DirectImport/smart', formData, config)

      console.log(`‚úÖ Upload completed for ${file.name}`)
      return response.data
    } catch (error) {
      console.error('Error uploading file:', error)
      if (error.code === 'ECONNABORTED') {
        throw new Error(`Upload timeout: File qu√° l·ªõn ho·∫∑c k·∫øt n·ªëi ch·∫≠m. Th·ª≠ l·∫°i v·ªõi file nh·ªè h∆°n.`)
      }
      throw new Error(`Direct Import failed: ${error.response?.data?.message || error.message}`)
    }
  }

  /**
   * Upload multiple files using Direct Import (tƒÉng t·ªëc 2-5x)
   * @param {FileList|Array} files - Files to upload
   * @param {string} category - The category of data (kh√¥ng d√πng n·ªØa)
   * @returns {Promise} Upload results
   */
  async uploadFiles(files, category = 'General') {
    try {
      const results = []

      // üöÄ Upload t·ª´ng file b·∫±ng Direct Import API song song
      const uploadPromises = Array.from(files).map(async file => {
        const formData = new FormData()
        formData.append('file', file)

        const response = await apiClient.post('/DirectImport/smart', formData, {
          headers: {
            'Content-Type': 'multipart/form-data',
          },
          timeout: 600000, // 10 ph√∫t cho file l·ªõn
        })

        return {
          fileName: file.name,
          result: response.data,
        }
      })

      const uploadResults = await Promise.all(uploadPromises)

      return {
        success: true,
        results: uploadResults,
        totalFiles: files.length,
        message: `Successfully uploaded ${files.length} files using Direct Import`,
      }
    } catch (error) {
      console.error('Error uploading files:', error)
      throw new Error(`Direct Import failed: ${error.response?.data?.message || error.message}`)
    }
  }

  /**
   * Get all imported data records
   * @returns {Promise<Array>} List of imported data records
   */
  async getImportedData() {
    try {
      const response = await apiClient.get('/DataImport')

      // Handle the .NET $values format
      if (response.data && response.data.$values) {
        return response.data.$values
      }

      return response.data || []
    } catch (error) {
      console.error('Error getting imported data:', error)
      throw new Error(`Failed to get imported data: ${error.response?.data?.message || error.message}`)
    }
  }

  /**
   * Get preview of imported data
   * @param {number} recordId - ID of the import record
   * @returns {Promise<Object>} Preview data with columns and records
   */
  async getDataPreview(recordId) {
    try {
      const response = await apiClient.get(`/DataImport/${recordId}/preview`)
      return response.data
    } catch (error) {
      console.error('Error getting data preview:', error)
      throw new Error(`Failed to get data preview: ${error.response?.data?.message || error.message}`)
    }
  }

  /**
   * Download original file
   * @param {number} recordId - ID of the import record
   * @returns {Promise} File download
   */
  async downloadOriginalFile(recordId) {
    try {
      const response = await apiClient.get(`/DataImport/${recordId}/download`, {
        responseType: 'blob',
      })

      // Create download link
      const url = window.URL.createObjectURL(new Blob([response.data]))
      const link = document.createElement('a')
      link.href = url

      // Get filename from response headers
      const contentDisposition = response.headers['content-disposition']
      let filename = 'download'
      if (contentDisposition) {
        const filenameMatch = contentDisposition.match(/filename="(.+)"/)
        if (filenameMatch) {
          filename = filenameMatch[1]
        }
      }

      link.setAttribute('download', filename)
      document.body.appendChild(link)
      link.click()
      link.remove()
      window.URL.revokeObjectURL(url)

      return { success: true, filename }
    } catch (error) {
      console.error('Error downloading file:', error)
      throw new Error(`Download failed: ${error.response?.data?.message || error.message}`)
    }
  }

  /**
   * Delete an imported data record
   * @param {number} recordId - ID of the record to delete
   * @returns {Promise} Delete result
   */
  async deleteData(recordId) {
    try {
      const response = await apiClient.delete(`/DataImport/${recordId}`)
      return response.data
    } catch (error) {
      console.error('Error deleting data:', error)
      throw new Error(`Delete failed: ${error.response?.data?.message || error.message}`)
    }
  }

  /**
   * Export all data to Excel
   * @returns {Promise} File download
   */
  async exportAllData() {
    try {
      const response = await apiClient.get('/DataImport/export', {
        responseType: 'blob',
      })

      // Create download link
      const url = window.URL.createObjectURL(new Blob([response.data]))
      const link = document.createElement('a')
      link.href = url

      // Generate filename with timestamp
      const timestamp = new Date().toISOString().slice(0, 19).replace(/[:-]/g, '')
      const filename = `KPI_Data_Export_${timestamp}.xlsx`

      link.setAttribute('download', filename)
      document.body.appendChild(link)
      link.click()
      link.remove()
      window.URL.revokeObjectURL(url)

      return { success: true, filename }
    } catch (error) {
      console.error('Error exporting data:', error)
      throw new Error(`Export failed: ${error.response?.data?.message || error.message}`)
    }
  }

  /**
   * Get KPI categories for data classification
   * @returns {Array} List of predefined KPI categories
   */
  getKpiCategories() {
    return [
      {
        id: 1,
        name: 'Ngu·ªìn v·ªën & D∆∞ n·ª£',
        icon: 'üí∞',
        description: 'T·ªïng ngu·ªìn v·ªën, d∆∞ n·ª£, ngu·ªìn v·ªën b√¨nh qu√¢n, d∆∞ n·ª£ b√¨nh qu√¢n',
        indicators: ['T·ªïng ngu·ªìn v·ªën', 'D∆∞ n·ª£', 'Ngu·ªìn v·ªën BQ', 'D∆∞ n·ª£ BQ'],
      },
      {
        id: 2,
        name: 'L·ª£i nhu·∫≠n & T√†i ch√≠nh',
        icon: 'üìà',
        description: 'L·ª£i nhu·∫≠n kho√°n t√†i ch√≠nh, c√°c ch·ªâ ti√™u hi·ªáu qu·∫£ kinh doanh',
        indicators: ['L·ª£i nhu·∫≠n kho√°n', 'ROA', 'ROE', 'NIM'],
      },
      {
        id: 3,
        name: 'Doanh thu D·ªãch v·ª•',
        icon: 'üè™',
        description: 'T·ªïng doanh thu ph√≠ d·ªãch v·ª•, c√°c lo·∫°i ph√≠ ng√¢n h√†ng',
        indicators: ['Doanh thu ph√≠ DV', 'Ph√≠ chuy·ªÉn kho·∫£n', 'Ph√≠ th·∫ª', 'Ph√≠ b·∫£o hi·ªÉm'],
      },
      {
        id: 4,
        name: 'Th·∫ª & S·∫£n ph·∫©m',
        icon: 'üí≥',
        description: 'S·ªë th·∫ª ph√°t h√†nh, c√°c s·∫£n ph·∫©m ng√¢n h√†ng ƒëi·ªán t·ª≠',
        indicators: ['S·ªë th·∫ª ph√°t h√†nh', 'Giao d·ªãch ATM', 'Internet Banking', 'Mobile Banking'],
      },
      {
        id: 5,
        name: 'Thu n·ª£ & R·ªßi ro',
        icon: '‚ö†Ô∏è',
        description: 'Thu n·ª£ ƒë√£ XLRR, c√°c ch·ªâ ti√™u qu·∫£n l√Ω r·ªßi ro t√≠n d·ª•ng',
        indicators: ['Thu n·ª£ XLRR', 'N·ª£ x·∫•u', 'T·ª∑ l·ªá NPL', 'D·ª± ph√≤ng r·ªßi ro'],
      },
      {
        id: 6,
        name: 'Kh√°ch h√†ng & Ti·∫øp th·ªã',
        icon: 'üë•',
        description: 'S·ªë l∆∞·ª£ng kh√°ch h√†ng m·ªõi, ch·ªâ ti√™u marketing v√† b√°n h√†ng',
        indicators: ['KH c√° nh√¢n m·ªõi', 'KH doanh nghi·ªáp m·ªõi', 'S·∫£n ph·∫©m cross-sell'],
      },
    ]
  }

  /**
   * Validate file before upload
   * @param {File} file - File to validate
   * @returns {Object} Validation result
   */
  validateFile(file) {
    const maxSize = 10 * 1024 * 1024 // 10MB
    const allowedTypes = [
      'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', // .xlsx
      'application/vnd.ms-excel', // .xls
      'text/csv', // .csv
      'application/pdf', // .pdf
    ]

    const allowedExtensions = /\.(xlsx|xls|csv|pdf)$/i

    if (!allowedTypes.includes(file.type) && !allowedExtensions.test(file.name)) {
      return {
        valid: false,
        error: 'File type not supported. Only Excel, CSV, and PDF files are allowed.',
      }
    }

    if (file.size > maxSize) {
      return {
        valid: false,
        error: 'File size exceeds 10MB limit.',
      }
    }

    return { valid: true }
  }

  /**
   * Format file size for display
   * @param {number} bytes - File size in bytes
   * @returns {string} Formatted file size
   */
  formatFileSize(bytes) {
    if (bytes === 0) return '0 Bytes'

    const k = 1024
    const sizes = ['Bytes', 'KB', 'MB', 'GB']
    const i = Math.floor(Math.log(bytes) / Math.log(k))

    return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i]
  }

  /**
   * Get file type icon
   * @param {string} mimeType - MIME type of the file
   * @returns {string} Icon emoji
   */
  getFileIcon(mimeType) {
    if (mimeType.includes('excel') || mimeType.includes('spreadsheet')) {
      return 'üìä'
    } else if (mimeType.includes('csv')) {
      return 'üìã'
    } else if (mimeType.includes('pdf')) {
      return 'üìÑ'
    }
    return 'üìÅ'
  }

  // üéØ SMART UPLOAD - Automatically choose best upload method based on file size
  async uploadFileSmart(file, category = 'General', onProgress = null) {
    const fileSizeMB = file.size / 1024 / 1024

    try {
      if (fileSizeMB < 50) {
        // Small files: regular upload
        console.log(`üì§ [SMART_UPLOAD] Using regular upload for ${file.name} (${fileSizeMB.toFixed(2)} MB)`)
        return await this.uploadFile(file, category, onProgress)
      } else if (fileSizeMB < 200) {
        // Medium files: streaming upload
        console.log(`üöÄ [SMART_UPLOAD] Using streaming upload for ${file.name} (${fileSizeMB.toFixed(2)} MB)`)
        return await this.uploadFileStreaming(file, onProgress)
      } else if (fileSizeMB < 500) {
        // Large files: chunked upload
        console.log(`üîÑ [SMART_UPLOAD] Using chunked upload for ${file.name} (${fileSizeMB.toFixed(2)} MB)`)
        return await this.uploadFileChunked(file, { onProgress })
      } else {
        // Very large files: parallel chunked upload
        console.log(`‚ö° [SMART_UPLOAD] Using parallel upload for ${file.name} (${fileSizeMB.toFixed(2)} MB)`)
        return await this.uploadFileParallel(file, { onProgress })
      }
    } catch (error) {
      console.error('‚ùå [SMART_UPLOAD] Smart upload error:', error)
      throw error
    }
  }

  // üöÄ STREAMING UPLOAD - For large files (50MB+)
  async uploadFileStreaming(file, onProgress = null) {
    try {
      const formData = new FormData()
      formData.append('file', file)

      const config = {
        headers: {
          'Content-Type': 'multipart/form-data',
        },
        timeout: 30 * 60 * 1000, // 30 minutes for streaming
        maxContentLength: Infinity,
        maxBodyLength: Infinity,
        onUploadProgress: progressEvent => {
          if (onProgress && progressEvent.total) {
            const percentCompleted = Math.round((progressEvent.loaded * 100) / progressEvent.total)
            onProgress(percentCompleted, progressEvent.loaded, progressEvent.total)
          }
        },
      }

      console.log(
        `üöÄ [STREAMING_UPLOAD] Starting streaming upload: ${file.name} (${(file.size / 1024 / 1024).toFixed(2)} MB)`
      )

      const response = await apiClient.post('/DirectImport/stream', formData, config)
      return response.data
    } catch (error) {
      console.error('‚ùå [STREAMING_UPLOAD] Upload error:', error)
      throw error
    }
  }

  // üîÑ CHUNKED UPLOAD - For very large files (100MB+) with resume capability
  async uploadFileChunked(file, options = {}) {
    try {
      const chunkedService = new ChunkedUploadService('/api/DirectImport')

      const uploadOptions = {
        onProgress: options.onProgress || (() => {}),
        onChunkProgress: options.onChunkProgress || (() => {}),
        chunkSize: options.chunkSize || 5 * 1024 * 1024, // 5MB chunks
      }

      console.log(
        `üîÑ [CHUNKED_UPLOAD] Starting chunked upload: ${file.name} (${(file.size / 1024 / 1024).toFixed(2)} MB)`
      )

      const result = await chunkedService.uploadFileChunked(file, uploadOptions)
      return result
    } catch (error) {
      console.error('‚ùå [CHUNKED_UPLOAD] Upload error:', error)
      throw error
    }
  }

  // üîÑ PARALLEL CHUNKED UPLOAD - For extremely large files with parallel processing
  async uploadFileParallel(file, options = {}) {
    try {
      const chunkedService = new ChunkedUploadService('/api/DirectImport')

      const uploadOptions = {
        onProgress: options.onProgress || (() => {}),
        maxConcurrent: options.maxConcurrent || 3,
        chunkSize: options.chunkSize || 10 * 1024 * 1024, // 10MB chunks for parallel
      }

      console.log(
        `üîÑ [PARALLEL_UPLOAD] Starting parallel upload: ${file.name} (${(file.size / 1024 / 1024).toFixed(2)} MB)`
      )

      const result = await chunkedService.uploadFileParallel(file, uploadOptions)
      return result
    } catch (error) {
      console.error('‚ùå [PARALLEL_UPLOAD] Upload error:', error)
      throw error
    }
  }

  // Resume interrupted chunked upload
  async resumeUpload(sessionId, file, options = {}) {
    try {
      const chunkedService = new ChunkedUploadService('/api/DirectImport')

      console.log(`üîÑ [RESUME_UPLOAD] Resuming upload for session: ${sessionId}`)

      const result = await chunkedService.resumeUpload(sessionId, file, options)
      return result
    } catch (error) {
      console.error('‚ùå [RESUME_UPLOAD] Resume error:', error)
      throw error
    }
  }

  // Cancel active upload
  async cancelUpload(uploadId) {
    try {
      const chunkedService = new ChunkedUploadService('/api/DirectImport')
      await chunkedService.cancelUpload(uploadId)

      console.log(`üö´ [CANCEL_UPLOAD] Cancelled upload: ${uploadId}`)
    } catch (error) {
      console.error('‚ùå [CANCEL_UPLOAD] Cancel error:', error)
      throw error
    }
  }

  // Get active uploads
  getActiveUploads() {
    const chunkedService = new ChunkedUploadService('/api/DirectImport')
    return chunkedService.getActiveUploads()
  }
}

// Create and export singleton instance
const dataImportService = new DataImportService()

export default dataImportService
export { DataImportService }
